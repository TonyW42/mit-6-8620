{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preliminaries"
      ],
      "metadata": {
        "id": "DjCYUrTdw0gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install jiwer\n",
        "!pip install wandb\n",
        "!git clone https://github.com/microsoft/MS-SNSD.git"
      ],
      "metadata": {
        "id": "h8UP5zDRo6Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjQ0WCF-hCvv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoProcessor, HubertModel, Wav2Vec2FeatureExtractor, HubertForCTC\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "from IPython.display import Audio, display\n",
        "import pickle\n",
        "from typing import Optional, Tuple, Union\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from jiwer import wer\n",
        "import wandb\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "import torch.nn.functional as F\n",
        "\n",
        "data_dir = './data'\n",
        "os.makedirs(data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self,\n",
        "                noise_scale = 1,\n",
        "                masked_speech_loss_ratio = 0.5,\n",
        "                masked_noise_loss_ratio = 0.5,\n",
        "                quantization_method = \"1\",\n",
        "                name = \"facebook/hubert-large-ls960-ft\",\n",
        "                use_contrastive_head = False,\n",
        "                emb_size_code = 768):\n",
        "        self.noise_scale = noise_scale\n",
        "        self.masked_speech_loss_ratio = masked_speech_loss_ratio\n",
        "        self.unmasked_speech_loss_ratio = 1 - self.masked_speech_loss_ratio\n",
        "        self.masked_noise_loss_ratio = masked_noise_loss_ratio\n",
        "        self.unmasked_noise_loss_ratio = 1 - self.masked_noise_loss_ratio\n",
        "        self.quantization_method = quantization_method\n",
        "        self.name = name\n",
        "        self.emb_size_code = emb_size_code\n",
        "        self.use_contrastive_head = use_contrastive_head\n",
        "        if quantization_method == \"1\":\n",
        "            self.speech_nclass = 500\n",
        "            self.noise_nclass = 500\n",
        "        elif quantization_method == \"1_2\":\n",
        "            self.speech_nclass = 500\n",
        "            self.noise_nclass = 100\n",
        "        elif quantization_method == \"2\":\n",
        "            self.speech_nclass = 600\n",
        "            self.noise_nclass = 600\n",
        "\n",
        "args = Config(use_contrastive_head = True)"
      ],
      "metadata": {
        "id": "aDGfcD3rjuMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "librispeech_dev = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"dev-clean\", download=True)\n",
        "librispeech_train = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
        "librispeech_test = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)"
      ],
      "metadata": {
        "id": "FPQShG8aiwRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1wVq5bdLcEjR-qKm2UOl1jCwD7hEAQsDj\n",
        "!gdown 1QHsrC52r8lhsHBPIy9Din0ir6CWM9nAf\n",
        "!gdown 1FzwfPHgvZoWSR4iwHdUX_SsuujCSib8o\n",
        "!gdown 160POl10uVDKhFfc_rdIlMZp-ylbkLLKm\n",
        "!gdown 13hUU13G5n-eCpDvhKuLCVK5x74c8ULvx\n",
        "!gdown 1FV4HYI0mVAgCfp5xgdLbhM6xbWd0xBY6\n",
        "!gdown 1ylPd1iH2HLUoxj_0mfjUSQ5kN1uD3P0D\n",
        "!gdown 1XdNYOMe62f5UG5_rVh6wy36RKjc-OSIe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-HO6Z24imOr",
        "outputId": "6b450392-38c7-4ee5-f553-1cff815c88d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wVq5bdLcEjR-qKm2UOl1jCwD7hEAQsDj\n",
            "To: /content/kmeans_labels.pkl\n",
            "100% 68.7M/68.7M [00:01<00:00, 60.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QHsrC52r8lhsHBPIy9Din0ir6CWM9nAf\n",
            "To: /content/kmeans_noise_500.pkl\n",
            "100% 3.39M/3.39M [00:00<00:00, 211MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FzwfPHgvZoWSR4iwHdUX_SsuujCSib8o\n",
            "To: /content/kmeans_signal_500.pkl\n",
            "100% 3.73M/3.73M [00:00<00:00, 200MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=160POl10uVDKhFfc_rdIlMZp-ylbkLLKm\n",
            "To: /content/kmeans_labels_1.pkl\n",
            "100% 68.6M/68.6M [00:00<00:00, 201MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13hUU13G5n-eCpDvhKuLCVK5x74c8ULvx\n",
            "To: /content/kmeans_all_500.pkl\n",
            "100% 4.67M/4.67M [00:00<00:00, 148MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FV4HYI0mVAgCfp5xgdLbhM6xbWd0xBY6\n",
            "To: /content/kmeans_labels_2.pkl\n",
            "100% 68.6M/68.6M [00:01<00:00, 43.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ylPd1iH2HLUoxj_0mfjUSQ5kN1uD3P0D\n",
            "To: /content/kmeans_noise_100.pkl\n",
            "100% 934k/934k [00:00<00:00, 156MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XdNYOMe62f5UG5_rVh6wy36RKjc-OSIe\n",
            "To: /content/kmeans_labels_1_2.pkl\n",
            "100% 68.6M/68.6M [00:00<00:00, 75.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  processor = AutoProcessor.from_pretrained(args.name)\n",
        "  model = HubertModel.from_pretrained(args.name)\n",
        "except:\n",
        "  processor = Wav2Vec2FeatureExtractor.from_pretrained(args.name)\n",
        "  model = HubertModel.from_pretrained(args.name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt_0wbrhiLGW",
        "outputId": "73cde4fd-6f3b-417f-e07b-c1753cdd10d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Quantization"
      ],
      "metadata": {
        "id": "h1zkqRWVw7Mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary functions"
      ],
      "metadata": {
        "id": "LlHykfQx-wi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ds_noise_mssnsd(torch.utils.data.Dataset):\n",
        "  def __init__(self, noise_dir, processor, factor = 1):\n",
        "    self.processor = processor\n",
        "    self.noise_dir = noise_dir\n",
        "    self.factor = factor\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(noise_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            if file_path.endswith(\".wav\"):\n",
        "              file_paths.append(file_path)\n",
        "\n",
        "    self.file_paths = file_paths\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    wave_file = self.file_paths[idx]\n",
        "    y, sample_rate = librosa.load(wave_file, sr=16000)\n",
        "    y = y * self.factor\n",
        "    encoded = self.processor(y[:200000], return_tensors=\"pt\", sampling_rate = sample_rate)\n",
        "    for k in encoded: encoded[k] = encoded[k].squeeze()\n",
        "    return encoded\n",
        "\n",
        "class ds_signal_mssnsd(torch.utils.data.Dataset):\n",
        "  def __init__(self, data, processor, leng = None):\n",
        "    self.data = data\n",
        "    self.processor = processor\n",
        "    self.leng = leng\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.leng is not None:\n",
        "      return self.leng\n",
        "    else:\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    clean_speech = self.data[idx][0].squeeze()\n",
        "    encoded = processor(clean_speech[:200000], return_tensors=\"pt\", sampling_rate = 16000)\n",
        "    for k in encoded: encoded[k] = encoded[k].squeeze()\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "Mf3IRi4PxlN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Loading exsiting quantization"
      ],
      "metadata": {
        "id": "-awv4oYm-XWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if args.quantization_method == \"1\":\n",
        "  file_name = \"kmeans_labels_1.pkl\"\n",
        "  noise_file = \"kmeans_noise_500.pkl\"\n",
        "  signal_file = \"kmeans_signal_500.pkl\"\n",
        "if args.quantization_method == \"1_2\":\n",
        "  file_name = \"kmeans_labels_1_2.pkl\"\n",
        "  noise_file = \"kmeans_noise_100.pkl\"\n",
        "  signal_file = \"kmeans_signal_500.pkl\"\n",
        "elif args.quantization_method == \"2\":\n",
        "  file_name = \"kmeans_labels_2.pkl\"\n",
        "  noise_file = \"kmeans_all_500.pkl\"\n",
        "  signal_file = \"kmeans_all_500.pkl\"\n",
        "\n",
        "with open(file_name, 'rb') as file:\n",
        "  kmeans_labels = pickle.load(file)\n",
        "with open(noise_file, 'rb') as file:\n",
        "  kmeans_noise = pickle.load(file)\n",
        "with open(signal_file, 'rb') as file:\n",
        "  kmeans_speech = pickle.load(file)\n",
        "\n",
        "tag_train = kmeans_labels[\"train\"]\n",
        "tag_test = kmeans_labels[\"test\"]\n",
        "tag_noise = kmeans_labels[\"noise\"]"
      ],
      "metadata": {
        "id": "0dQn_oDa-0Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training"
      ],
      "metadata": {
        "id": "TbXNQFBa47hI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 build pe-training model"
      ],
      "metadata": {
        "id": "Y58Xr08diNZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Modify huggingface function"
      ],
      "metadata": {
        "id": "9uoBQLZfpQjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_mask_indices(\n",
        "    shape: Tuple[int, int],\n",
        "    mask_prob: float,\n",
        "    mask_length: int,\n",
        "    attention_mask: Optional[torch.LongTensor] = None,\n",
        "    min_masks: int = 0,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n",
        "    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n",
        "    CPU as part of the preprocessing during training.\n",
        "\n",
        "    Args:\n",
        "        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n",
        "               the first element is the batch size and the second element is the length of the axis to span.\n",
        "        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n",
        "                    independently generated mask spans of length `mask_length` is computed by\n",
        "                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n",
        "                    actual percentage will be smaller.\n",
        "        mask_length: size of the mask\n",
        "        min_masks: minimum number of masked spans\n",
        "        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n",
        "                        each batch dimension.\n",
        "    \"\"\"\n",
        "    batch_size, sequence_length = shape\n",
        "\n",
        "    if mask_length < 1:\n",
        "        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n",
        "\n",
        "    if mask_length > sequence_length:\n",
        "        raise ValueError(\n",
        "            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n",
        "            f\" and `sequence_length`: {sequence_length}`\"\n",
        "        )\n",
        "\n",
        "    # epsilon is used for probabilistic rounding\n",
        "    epsilon = np.random.rand(1).item()\n",
        "\n",
        "    def compute_num_masked_span(input_length):\n",
        "        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n",
        "        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n",
        "        num_masked_span = max(num_masked_span, min_masks)\n",
        "\n",
        "        # make sure num masked span <= sequence_length\n",
        "        if num_masked_span * mask_length > sequence_length:\n",
        "            num_masked_span = sequence_length // mask_length\n",
        "\n",
        "        # make sure num_masked span is also <= input_length - (mask_length - 1)\n",
        "        if input_length - (mask_length - 1) < num_masked_span:\n",
        "            num_masked_span = max(input_length - (mask_length - 1), 0)\n",
        "\n",
        "        return num_masked_span\n",
        "\n",
        "    # compute number of masked spans in batch\n",
        "    input_lengths = (\n",
        "        attention_mask.sum(-1).detach().tolist()\n",
        "        if attention_mask is not None\n",
        "        else [sequence_length for _ in range(batch_size)]\n",
        "    )\n",
        "\n",
        "    # SpecAugment mask to fill\n",
        "    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n",
        "    spec_aug_mask_idxs = []\n",
        "\n",
        "    max_num_masked_span = compute_num_masked_span(sequence_length)\n",
        "\n",
        "    if max_num_masked_span == 0:\n",
        "        return spec_aug_mask\n",
        "\n",
        "    for input_length in input_lengths:\n",
        "        # compute num of masked spans for this input\n",
        "        num_masked_span = compute_num_masked_span(input_length)\n",
        "\n",
        "        # get random indices to mask\n",
        "        spec_aug_mask_idx = np.random.choice(\n",
        "            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n",
        "        )\n",
        "\n",
        "        # pick first sampled index that will serve as a dummy index to pad vector\n",
        "        # to ensure same dimension for all batches due to probabilistic rounding\n",
        "        # Picking first sample just pads those vectors twice.\n",
        "        if len(spec_aug_mask_idx) == 0:\n",
        "            # this case can only happen if `input_length` is strictly smaller then\n",
        "            # `sequence_length` in which case the last token has to be a padding\n",
        "            # token which we can use as a dummy mask id\n",
        "            dummy_mask_idx = sequence_length - 1\n",
        "        else:\n",
        "            dummy_mask_idx = spec_aug_mask_idx[0]\n",
        "\n",
        "        spec_aug_mask_idx = np.concatenate(\n",
        "            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n",
        "        )\n",
        "        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n",
        "\n",
        "    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n",
        "\n",
        "    # expand masked indices to masked spans\n",
        "    spec_aug_mask_idxs = np.broadcast_to(\n",
        "        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n",
        "    )\n",
        "    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n",
        "\n",
        "    # add offset to the starting indexes so that indexes now create a span\n",
        "    offsets = np.arange(mask_length)[None, None, :]\n",
        "    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n",
        "        batch_size, max_num_masked_span * mask_length\n",
        "    )\n",
        "    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n",
        "\n",
        "    # ensure that we cannot have indices larger than sequence_length\n",
        "    if spec_aug_mask_idxs.max() > sequence_length - 1:\n",
        "        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n",
        "\n",
        "    # scatter indices to mask\n",
        "    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n",
        "\n",
        "    return spec_aug_mask\n",
        "\n",
        "class masked_hubert(type(model)):\n",
        "    def _mask_hidden_states(\n",
        "          self,\n",
        "          hidden_states: torch.FloatTensor,\n",
        "          mask_time_indices: Optional[torch.FloatTensor] = None,\n",
        "          attention_mask: Optional[torch.LongTensor] = None,\n",
        "      ):\n",
        "        \"\"\"\n",
        "        Masks extracted features along time axis and/or along feature axis according to\n",
        "        [SpecAugment](https://arxiv.org/abs/1904.08779).\n",
        "        \"\"\"\n",
        "\n",
        "        # `config.apply_spec_augment` can set masking to False\n",
        "        if not getattr(self.config, \"apply_spec_augment\", True):\n",
        "            return hidden_states\n",
        "\n",
        "        # generate indices & apply SpecAugment along time axis\n",
        "        batch_size, sequence_length, hidden_size = hidden_states.size()\n",
        "\n",
        "        if mask_time_indices is not None:\n",
        "            # apply SpecAugment along time axis with given mask_time_indices\n",
        "            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)\n",
        "        elif self.config.mask_time_prob > 0 and self.training:\n",
        "            mask_time_indices = _compute_mask_indices(\n",
        "                (batch_size, sequence_length),\n",
        "                mask_prob=self.config.mask_time_prob,\n",
        "                mask_length=self.config.mask_time_length,\n",
        "                attention_mask=attention_mask,\n",
        "                min_masks=self.config.mask_time_min_masks,\n",
        "            )\n",
        "            mask_time_indices = torch.tensor(mask_time_indices, device=hidden_states.device, dtype=torch.bool)\n",
        "            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)\n",
        "\n",
        "        if self.config.mask_feature_prob > 0 and self.training:\n",
        "            # generate indices & apply SpecAugment along feature axis\n",
        "            mask_feature_indices = _compute_mask_indices(\n",
        "                (batch_size, hidden_size),\n",
        "                mask_prob=self.config.mask_feature_prob,\n",
        "                mask_length=self.config.mask_feature_length,\n",
        "                min_masks=self.config.mask_feature_min_masks,\n",
        "            )\n",
        "            mask_feature_indices = torch.tensor(mask_feature_indices, device=hidden_states.device, dtype=torch.bool)\n",
        "            mask_feature_indices = mask_feature_indices[:, None].expand(-1, sequence_length, -1)\n",
        "            hidden_states[mask_feature_indices] = 0\n",
        "\n",
        "        return hidden_states, mask_time_indices\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_values: Optional[torch.Tensor],\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        mask_time_indices: Optional[torch.FloatTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) :\n",
        "        \"\"\"\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```python\n",
        "        >>> from transformers import AutoProcessor, HubertModel\n",
        "        >>> from datasets import load_dataset\n",
        "        >>> import soundfile as sf\n",
        "\n",
        "        >>> processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "        >>> model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "\n",
        "\n",
        "        >>> def map_to_array(batch):\n",
        "        ...     speech, _ = sf.read(batch[\"file\"])\n",
        "        ...     batch[\"speech\"] = speech\n",
        "        ...     return batch\n",
        "\n",
        "\n",
        "        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "        >>> ds = ds.map(map_to_array)\n",
        "\n",
        "        >>> input_values = processor(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n",
        "        >>> hidden_states = model(input_values).last_hidden_state\n",
        "        ```\"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        extract_features = self.feature_extractor(input_values)\n",
        "        extract_features = extract_features.transpose(1, 2)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # compute reduced attention_mask corresponding to feature vectors\n",
        "            attention_mask = self._get_feature_vector_attention_mask(extract_features.shape[1], attention_mask)\n",
        "\n",
        "        hidden_states = self.feature_projection(extract_features)\n",
        "        hidden_states, mask_time_index = self._mask_hidden_states(hidden_states, mask_time_indices=mask_time_indices)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = encoder_outputs[0]\n",
        "\n",
        "        if not return_dict:\n",
        "            return (hidden_states,) + encoder_outputs[1:]\n",
        "\n",
        "        return {\n",
        "            \"last_hidden_state\" : hidden_states,\n",
        "            \"hidden_states\" : encoder_outputs.hidden_states,\n",
        "            \"attentions\" : encoder_outputs.attentions,\n",
        "            \"mask_time_index\" : mask_time_index,\n",
        "        }"
      ],
      "metadata": {
        "id": "Aij5IInZCU7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 Other functions"
      ],
      "metadata": {
        "id": "ioJ_1rmQpWL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class hubert_pretrain(torch.nn.Module):\n",
        "    def __init__(self, model, args, kmeans_noise = None, kmeans_speech = None):\n",
        "        super(hubert_pretrain, self).__init__()\n",
        "        self.model = model\n",
        "        self.emb_size = self.model.config.hidden_size\n",
        "        self.args = args\n",
        "        self.kmeans_noise = kmeans_noise\n",
        "        self.kmeans_speech = kmeans_speech\n",
        "        speech_out_size = args.speech_nclass if not args.use_contrastive_head else args.emb_size_code\n",
        "        noise_out_size = args.noise_nclass if not args.use_contrastive_head else args.emb_size_code\n",
        "        self.linear_noise = torch.nn.Linear(self.emb_size, noise_out_size)\n",
        "        self.linear_speech = torch.nn.Linear(self.emb_size, speech_out_size)\n",
        "        self.tau = 0.01\n",
        "        # assert kmeans_noise is not None and kmeans_speech is not None\n",
        "\n",
        "    def cos_sim(self, cluster_center, logits):\n",
        "\n",
        "        # Reshape tensor2 for broadcasting: from [num_class, emb_size] to [1, num_class, emb_size]\n",
        "        tensor2 = cluster_center.unsqueeze(0)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        # tensor1 shape: [bs, seq_len, emb_size]\n",
        "        # tensor2 shape: [1, num_class, emb_size]\n",
        "        # We need tensor1 to be [bs, seq_len, 1, emb_size] and tensor2 to be [1, 1, num_class, emb_size] for broadcasting\n",
        "        tensor1_expanded = logits.unsqueeze(2)  # Now [bs, seq_len, 1, emb_size]\n",
        "        tensor2_expanded = tensor2.unsqueeze(0)  # Now [1, 1, num_class, emb_size]\n",
        "\n",
        "        # Calculate cosine similarity over the last dimension (emb_size)\n",
        "        cos_sim = F.cosine_similarity(tensor1_expanded, tensor2_expanded, dim=3)\n",
        "\n",
        "        # cos_sim shape will be [bs, seq_len, num_class]\n",
        "        return cos_sim\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        encoded = self.model(data.input_values.to(device), data.attention_mask.to(device))\n",
        "        noise_logits = self.linear_noise(encoded[\"last_hidden_state\"])\n",
        "        speech_logits = self.linear_speech(encoded[\"last_hidden_state\"])\n",
        "        mask_idx = encoded[\"mask_time_index\"]\n",
        "        # print(\"*\" * 20)\n",
        "        if self.args.use_contrastive_head:\n",
        "            noise_center = torch.tensor(self.kmeans_noise.cluster_centers_).to(device)\n",
        "            speech_center = torch.tensor(self.kmeans_speech.cluster_centers_).to(device)\n",
        "            # print(\"=\" * 20)\n",
        "            noise_logits = self.cos_sim(noise_center,  noise_logits) /  self.tau\n",
        "            # print(\"---\" * 20)\n",
        "            speech_logits = self.cos_sim(speech_center, speech_logits) / self.tau\n",
        "            # print(\"//\" * 20)\n",
        "        return noise_logits, speech_logits, mask_idx"
      ],
      "metadata": {
        "id": "LV7BLyFYI2PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class pre_train_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, processor, noise_dir, data_label, noise_label, args):\n",
        "        self.data = data\n",
        "        self.processor = processor\n",
        "        self.noise_dir = noise_dir\n",
        "        self.data_label = data_label\n",
        "        self.noise_label = noise_label\n",
        "        file_paths = []\n",
        "        self.args = args\n",
        "        for root, dirs, files in os.walk(noise_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                if file_path.endswith(\".wav\"):\n",
        "                    file_paths.append(file_path)\n",
        "\n",
        "        self.file_paths = file_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def pad_label(self, label, leng):\n",
        "        result = torch.full(size = (leng, ), fill_value = -100)\n",
        "        for i in range(min(len(result), len(label))):\n",
        "            result[i] = label[i]\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clean_speech = self.data[idx][0].squeeze()\n",
        "        len_noise = 0\n",
        "        noise = None\n",
        "        noise_label = None\n",
        "        ## concatenate noise if noise if noise is shorter than speech\n",
        "        #############################################################################\n",
        "        while len_noise < len(clean_speech) or noise is None:\n",
        "            noise_idx = np.random.choice(len(self.file_paths))\n",
        "            noise_path = self.file_paths[noise_idx]\n",
        "            noise_tmp, sample_rate = librosa.load(noise_path, sr=16000)\n",
        "            if noise is None:\n",
        "                noise = noise_tmp\n",
        "                noise_label = self.noise_label[noise_idx]\n",
        "            else:\n",
        "                noise = np.concatenate((noise, noise_tmp))\n",
        "                noise_label = np.concatenate((noise_label, self.noise_label[noise_idx]))\n",
        "            len_noise = len(noise)\n",
        "        #############################################################################\n",
        "\n",
        "        noisy_speech = clean_speech  + noise[:len(clean_speech)] * self.args.noise_scale\n",
        "        encoded = processor(\n",
        "            noisy_speech[:200000], return_tensors=\"pt\",\n",
        "            sampling_rate = 16000,padding = \"max_length\",\n",
        "            max_length = 200000, truncation=True,\n",
        "            return_attention_mask = True\n",
        "                            )\n",
        "        encoded[\"speech_label\"] = self.pad_label(self.data_label[idx], 624)\n",
        "        encoded[\"noise_label\"] = self.pad_label(noise_label, 624)\n",
        "        for k in encoded: encoded[k] = encoded[k].squeeze()\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "PMwuTggrVubS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Train function"
      ],
      "metadata": {
        "id": "8LA9Pro7iUUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9-TjoJObyLc",
        "outputId": "4f97b1de-9a58-4b8b-c10a-89985e2be7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    wandb.init(project=\"hubert_pretrain_1\", entity=\"yilin_wang\")\n",
        "    hubert_masked = masked_hubert(model.config)\n",
        "    hubert_masked.load_state_dict(model.state_dict())\n",
        "    hubert_pt = hubert_pretrain(hubert_masked, args, kmeans_noise, kmeans_speech)\n",
        "    hubert_pt = hubert_pt.to(device)\n",
        "    # return hubert_pt ## delete this\n",
        "    train_dataset = pre_train_dataset(librispeech_train, processor,\n",
        "                                      noise_dir = \"/content/MS-SNSD/noise_train/\",\n",
        "                                      data_label = tag_train,\n",
        "                                      noise_label = tag_noise,\n",
        "                                      args = args)\n",
        "    test_dataset = pre_train_dataset(librispeech_test, processor,\n",
        "                                      noise_dir = \"/content/MS-SNSD/noise_test/\",\n",
        "                                      data_label = tag_test,\n",
        "                                      noise_label = tag_noise,\n",
        "                                      args = args)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = 4, shuffle = True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size = 4, shuffle = False)\n",
        "    optimizer = torch.optim.Adam(hubert_masked.parameters(), lr = 1e-5)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(10):\n",
        "        hubert_pt.train()\n",
        "        tbar_train = tqdm(train_dataloader, position=0, leave=True)\n",
        "        for batch in tbar_train:\n",
        "            optimizer.zero_grad()\n",
        "            noise_logits, speech_logits, mask_idx = hubert_pt(batch)\n",
        "            speech_label_masked = batch[\"speech_label\"].clone().to(device)\n",
        "            speech_label_unmasked = batch[\"speech_label\"].clone().to(device)\n",
        "            noise_label_masked = batch[\"noise_label\"].clone().to(device)\n",
        "            noise_label_unmasked = batch[\"noise_label\"].clone().to(device)\n",
        "            # print(speech_label_masked.shape)\n",
        "            # print(mask_idx.shape)\n",
        "            # print(speech_logits.shape)\n",
        "            speech_label_masked[~mask_idx] = -100\n",
        "            noise_label_masked[~mask_idx] = -100\n",
        "            speech_label_unmasked[mask_idx] = -100\n",
        "            noise_label_unmasked[mask_idx] = -100\n",
        "            loss_speech_masked = criterion(speech_logits.view(-1, 500), speech_label_masked.view(-1))\n",
        "            loss_noise_masked = criterion(noise_logits.view(-1, 500), noise_label_masked.view(-1))\n",
        "            loss_speech_unmasked = criterion(speech_logits.view(-1, 500), speech_label_unmasked.view(-1))\n",
        "            loss_noise_unmasked = criterion(noise_logits.view(-1, 500), noise_label_unmasked.view(-1))\n",
        "            ## add weights here\n",
        "            loss = args.masked_speech_loss_ratio * loss_speech_masked + \\\n",
        "                  args.unmasked_speech_loss_ratio * loss_speech_unmasked + \\\n",
        "                  args.masked_noise_loss_ratio * loss_noise_masked + \\\n",
        "                  args.unmasked_noise_loss_ratio * loss_noise_unmasked\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tbar_train.set_postfix(loss=loss.item())\n",
        "            wandb.log({\"loss\": loss.item(),\n",
        "                      \"loss_speech_masked\": loss_speech_masked.item(),\n",
        "                      \"loss_noise_masked\": loss_noise_masked.item(),\n",
        "                      \"loss_speech_unmasked\": loss_speech_unmasked.item(),\n",
        "                      \"loss_noise_unmasked\": loss_noise_unmasked.item()})\n",
        "    wandb.finish()\n",
        "    return hubert_pt\n",
        "\n",
        "\n",
        "\n",
        "hubert_trained = train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "xDFQaS5iI-9Z",
        "outputId": "3b818c85-84ac-4700-b645-894a12204a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myilin_wang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240509_190006-lq8h3eg6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yilin_wang/hubert_pretrain_1/runs/lq8h3eg6' target=\"_blank\">honest-tree-11</a></strong> to <a href='https://wandb.ai/yilin_wang/hubert_pretrain_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/yilin_wang/hubert_pretrain_1' target=\"_blank\">https://wandb.ai/yilin_wang/hubert_pretrain_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yilin_wang/hubert_pretrain_1/runs/lq8h3eg6' target=\"_blank\">https://wandb.ai/yilin_wang/hubert_pretrain_1/runs/lq8h3eg6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/7135 [00:02<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************\n",
            "====================\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 7.14 GiB. GPU 0 has a total capacity of 39.56 GiB of which 5.27 GiB is free. Process 82376 has 34.28 GiB memory in use. Of the allocated memory 30.16 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8cc9615dc555>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mhubert_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8cc9615dc555>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtbar_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mnoise_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeech_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhubert_pt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mspeech_label_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"speech_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mspeech_label_unmasked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"speech_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-15662956ddd1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mnoise_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_center\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnoise_logits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m       \u001b[0mspeech_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeech_logits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnoise_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeech_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-15662956ddd1>\u001b[0m in \u001b[0;36mcos_sim\u001b[0;34m(self, cluster_center, logits)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Calculate cosine similarity over the last dimension (emb_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# cos_sim shape will be [bs, seq_len, num_class]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.14 GiB. GPU 0 has a total capacity of 39.56 GiB of which 5.27 GiB is free. Process 82376 has 34.28 GiB memory in use. Of the allocated memory 30.16 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluation on ASR"
      ],
      "metadata": {
        "id": "g0jyVzGDlEU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class asr_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, processor, noise_dir, data_label, noise_label, leng = None):\n",
        "        self.data = data\n",
        "        self.processor = processor\n",
        "        self.noise_dir = noise_dir\n",
        "        self.data_label = data_label\n",
        "        self.noise_label = noise_label\n",
        "        self.leng = leng\n",
        "        file_paths = []\n",
        "        for root, dirs, files in os.walk(noise_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                if file_path.endswith(\".wav\"):\n",
        "                  file_paths.append(file_path)\n",
        "\n",
        "        self.file_paths = file_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.leng is not None:\n",
        "            return self.leng\n",
        "        return len(self.data)\n",
        "\n",
        "    def pad_label(self, label, leng):\n",
        "        result = torch.full(size = (leng, ), fill_value = -100)\n",
        "        for i in range(min(len(result), len(label))):\n",
        "            result[i] = label[i]\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clean_speech = self.data[idx][0].squeeze()\n",
        "        len_noise = 0\n",
        "        noise = None\n",
        "        noise_label = None\n",
        "        ## concatenate noise if noise if noise is shorter than speech\n",
        "        #############################################################################\n",
        "        while len_noise < len(clean_speech) or noise is None:\n",
        "            noise_idx = np.random.choice(len(self.file_paths))\n",
        "            noise_path = self.file_paths[noise_idx]\n",
        "            noise_tmp, sample_rate = librosa.load(noise_path, sr=16000)\n",
        "            if noise is None:\n",
        "                noise = noise_tmp\n",
        "                noise_label = self.noise_label[noise_idx]\n",
        "            else:\n",
        "                noise = np.concatenate((noise, noise_tmp))\n",
        "                noise_label = np.concatenate((noise_label, self.noise_label[noise_idx]))\n",
        "            len_noise = len(noise)\n",
        "        #############################################################################\n",
        "\n",
        "        noisy_speech = clean_speech  + noise[:len(clean_speech)]\n",
        "        encoded = processor(\n",
        "            noisy_speech[:200000], return_tensors=\"pt\",\n",
        "            sampling_rate = 16000,#padding = \"max_length\",\n",
        "            # max_length = 200000, truncation=True,\n",
        "                            )\n",
        "        encoded[\"labels\"] = processor(text=librispeech_test[idx][2], return_tensors=\"pt\").input_ids\n",
        "        for k in encoded: encoded[k] = encoded[k].squeeze()\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "LB7vbifvx3re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_asr(hubert_base):\n",
        "    hubert_ctc = HubertForCTC.from_pretrained(args.name)\n",
        "    hubert_ctc.hubert = hubert_base\n",
        "    hubert_ctc.to(device)\n",
        "    train_dataset = asr_dataset(librispeech_train, processor,\n",
        "                                      noise_dir = \"/content/MS-SNSD/noise_train/\",\n",
        "                                      data_label = tag_train,\n",
        "                                      noise_label = tag_noise)\n",
        "    test_dataset = asr_dataset(librispeech_test, processor,\n",
        "                                      noise_dir = \"/content/MS-SNSD/noise_test/\",\n",
        "                                      data_label = tag_test,\n",
        "                                      noise_label = tag_noise)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = 1, shuffle = False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle = False)\n",
        "    optimizer = torch.optim.Adam(hubert_ctc.parameters(), lr = 1e-5)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        tbar_train = tqdm(train_dataloader, position=0, leave=True)\n",
        "        for batch in tbar_train:\n",
        "            optimizer.zero_grad()\n",
        "            encoded = hubert_ctc(batch.input_values.to(device),\n",
        "                                labels = batch.labels.to(device)\n",
        "                                )\n",
        "            loss = encoded.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tbar_train.set_postfix(loss=loss.item())\n",
        "    return hubert_ctc\n",
        "\n",
        "base_model = HubertModel.from_pretrained(args.name)\n",
        "base_model.load_state_dict(hubert_trained.model.state_dict())\n",
        "\n",
        "hubert_ctc = train_asr(base_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnxfC_rJzd_H",
        "outputId": "4594b91a-f6a2-4e4b-e090-220a2c5b297b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  1%|          | 250/28539 [01:18<2:27:26,  3.20it/s, loss=nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_asr(model, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "    with torch.no_grad():\n",
        "        tbar_loader = tqdm(loader, position=0, leave=True)\n",
        "        for batch in tbar_loader:\n",
        "            input_values = batch['input_values'].to(device)\n",
        "            # attention_mask = batch.get('attention_mask').to(device) if 'attention_mask' in batch else None\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_values).logits\n",
        "\n",
        "            # Decode model output to text\n",
        "            pred_ids = torch.argmax(logits, dim=-1)\n",
        "            batch_predictions = processor.batch_decode(pred_ids)\n",
        "            batch_references = processor.batch_decode(labels)\n",
        "\n",
        "            # Store predictions and references for WER calculation\n",
        "            predictions.extend(batch_predictions)\n",
        "            references.extend(batch_references)\n",
        "\n",
        "        overall_wer = wer(references, predictions)\n",
        "        return overall_wer\n"
      ],
      "metadata": {
        "id": "bTSSwZGUX2s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hubert_ctc = HubertForCTC.from_pretrained(name)\n",
        "hubert_ctc.to(device)\n",
        "train_dataset = asr_dataset(librispeech_train, processor,\n",
        "                                  noise_dir = \"/content/MS-SNSD/noise_train/\",\n",
        "                                  data_label = tag_train,\n",
        "                                  noise_label = tag_noise)\n",
        "test_dataset = asr_dataset(librispeech_test, processor,\n",
        "                                  noise_dir = \"/content/MS-SNSD/noise_test/\",\n",
        "                                  data_label = tag_test,\n",
        "                                  noise_label = tag_noise,\n",
        "                                  leng = 500)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 1, shuffle = False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jGVOQgforF0",
        "outputId": "5ce91687-94b9-4251-94a6-2e27d0937db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_asr(hubert_ctc, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JMffUw7owfk",
        "outputId": "4a60fcab-90a7-4ecf-a9a9-6257aaa3b915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 500/500 [00:31<00:00, 16.03it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6263713729108992"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}